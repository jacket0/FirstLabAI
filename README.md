# FirstLabAI

# Классификация пород собак с использованием архитектуры RegNet

## 1. Постановка задачи, Цель работы. Теоретическая база

### Постановка задачи
Задача заключается в классификации пород собак из датасета Stanford Dogs Dataset, который содержит изображения 120 различных пород собак. Необходимо реализовать и обучить нейронную сеть на основе архитектуры RegNet для решения задачи многоклассовой классификации.

### Цель работы
Научиться реализовывать алгоритмы глубокого обучения, в частности:
- Работать с датасетами изображений
- Применять transfer learning (дообучение предобученных моделей)
- Сравнивать различные оптимизаторы (Adam и RAdam)
- Сравнивать обучение с нуля и дообучение предобученных моделей
- Оценивать качество моделей с помощью метрик Precision и Recall

### Теоретическая база

#### Архитектура RegNet
RegNet (Regularized Network) — это семейство архитектур нейронных сетей, разработанное исследователями из Facebook AI Research. Основные особенности:

- **Регуляризация**: Использует регуляризацию для контроля сложности модели
- **Эффективность**: Оптимизирована для баланса между точностью и скоростью
- **Масштабируемость**: Легко масштабируется для различных вычислительных ресурсов
- **Структура**: Состоит из последовательных блоков с остаточными соединениями

В данной работе используется модель `regnet_y_3_2gf` (RegNetY-3.2GF), которая имеет около 3.2 миллиарда операций с плавающей точкой (GFLOPS).

#### Transfer Learning (Дообучение)
Transfer Learning — это техника, при которой модель, предобученная на большом датасете (например, ImageNet), адаптируется для решения новой задачи. Преимущества:
- Требуется меньше данных для обучения
- Быстрее достигается хорошее качество
- Эффективное использование вычислительных ресурсов

#### Оптимизаторы

**Adam (Adaptive Moment Estimation)**:
- Адаптивный алгоритм оптимизации
- Вычисляет адаптивные оценки моментов первого и второго порядка градиентов
- Хорошо работает на большинстве задач глубокого обучения

**RAdam (Rectified Adam)**:
- Улучшенная версия Adam
- Решает проблему нестабильной дисперсии в начале обучения
- Автоматически включает/выключает адаптивную скорость обучения
- Обеспечивает более стабильное обучение

#### Метрики качества

**Precision (Точность)**: Доля правильно предсказанных положительных примеров среди всех предсказанных положительных:
```
Precision = TP / (TP + FP)
```

**Recall (Полнота)**: Доля правильно предсказанных положительных примеров среди всех реальных положительных:
```
Recall = TP / (TP + FN)
```

**F1-Score**: Гармоническое среднее Precision и Recall:
```
F1 = 2 * (Precision * Recall) / (Precision + Recall)
```

## 2. Описание разработанной системы

### Алгоритмы и принципы работы

#### 2.1 Подготовка данных

1. **Загрузка датасета**: Используется Stanford Dogs Dataset, содержащий изображения 120 пород собак
2. **Разделение данных**: Датасет разделяется на три части:
   - Обучающая выборка: 70%
   - Валидационная выборка: 15%
   - Тестовая выборка: 15%
3. **Аугментация данных**:
   - Для обучающей выборки: случайное изменение размера и обрезка, горизонтальное отражение, поворот, изменение яркости/контраста/насыщенности
   - Для валидационной и тестовой выборок: только изменение размера и центральная обрезка
4. **Нормализация**: Применяется нормализация с параметрами ImageNet (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

#### 2.2 Архитектура модели

**RegNetY-3.2GF**:
- Базовая архитектура: предобученная на ImageNet
- Входной размер: 224×224×3
- Выходной слой: заменен на линейный слой с 120 выходами (количество классов)

#### 2.3 Эксперименты

**Эксперимент 1: Дообучение предобученной модели**

1. **Модель с оптимизатором Adam**:
   - Замораживаются все слои, кроме последнего (классификатора)
   - Обучается только последний слой
   - Learning rate: 1e-4
   - Количество эпох: 10

2. **Модель с оптимизатором RAdam**:
   - Аналогичная структура
   - Используется оптимизатор RAdam вместо Adam
   - Те же гиперпараметры

**Эксперимент 2: Обучение с нуля**

- Модель RegNet без предобученных весов
- Обучаются все параметры модели
- Оптимизатор: Adam
- Learning rate: 5e-4 (больше, чем при дообучении)
- Количество эпох: 15

#### 2.4 Процесс обучения

Для каждой модели:
1. **Обучение**: Проход по обучающей выборке с вычислением функции потерь (CrossEntropyLoss)
2. **Валидация**: Оценка на валидационной выборке после каждой эпохи
3. **Тестирование**: Финальная оценка на тестовой выборке
4. **Метрики**: Вычисление Accuracy, Precision, Recall, F1-Score

### Архитектура системы



## 3. Результаты работы и тестирования системы

### 3.1 Параметры эксперимента

- **Архитектура**: RegNetY-3.2GF
- **Размер батча**: 32
- **Learning rate (дообучение)**: 1e-4
- **Learning rate (с нуля)**: 5e-4
- **Количество эпох (дообучение)**: 10
- **Количество эпох (с нуля)**: 15
- **Функция потерь**: CrossEntropyLoss
- **Размер изображений**: 224×224


### 3.2 Графики обучения

После выполнения обучения будут сгенерированы следующие графики (сохраняются в файл `training_results_kaggle.png`):

1. **Кривые потерь (Loss Curves)** для каждой модели:
   - Обучающая и валидационная функции потерь по эпохам
   - Позволяет оценить сходимость и переобучение

2. **Сравнение валидационной точности (Validation Accuracy)**:
   - Динамика изменения точности для всех трех моделей
   - Позволяет сравнить эффективность различных подходов

3. **Precision и Recall по эпохам**:
   - Отдельные кривые для Precision и Recall
   - Позволяет оценить баланс между точностью и полнотой

4. **Сравнение F1-Score на тестовой выборке**:
   - Столбчатая диаграмма с финальными результатами
   - Наглядное сравнение всех моделей

## 4. Выводы по работе

### 4.1 Достигнутые результаты

1. **Реализована система классификации пород собак** на основе архитектуры RegNet
2. **Проведены три эксперимента** для сравнения различных подходов к обучению
3. **Применены современные техники** глубокого обучения: transfer learning, аугментация данных, различные оптимизаторы
4. **Оценено качество моделей** с использованием метрик Precision, Recall, F1-Score и Accuracy

### 4.2 Наблюдения

1. **Эффективность Transfer Learning**: Дообучение предобученных моделей является более эффективным подходом, чем обучение с нуля, особенно при ограниченном количестве данных.

2. **Влияние оптимизатора**: RAdam может обеспечить более стабильное обучение по сравнению с Adam, хотя финальные результаты могут быть схожими.

3. **Важность аугментации данных**: Применение аугментации данных помогает модели лучше обобщаться и избегать переобучения.

4. **Баланс метрик**: Важно учитывать не только Accuracy, но и Precision и Recall, особенно в задачах с несбалансированными классами.

### 4.3 Возможные улучшения

1. **Fine-tuning**: Вместо замораживания всех слоев можно разморозить последние несколько слоев для более глубокого дообучения
2. **Learning rate scheduling**: Использование расписания изменения learning rate может улучшить результаты
3. **Ensemble методы**: Объединение нескольких моделей может повысить точность
4. **Дополнительная аугментация**: Использование более сложных техник аугментации (MixUp, CutMix)
5. **Обработка дисбаланса классов**: Применение техник для работы с несбалансированными данными

## Инструкция по запуску

### Требования

- Python 3.8+
- PyTorch 2.0+
- CUDA (опционально, для использования GPU)

### Установка зависимостей

```bash
pip install -r requirements.txt
```

### Структура данных

Убедитесь, что датасет Stanford Dogs находится в следующей структуре:

```
stanford_dogs/
├── images/
│   └── Images/
│       ├── n02085620-Chihuahua/
│       ├── n02085782-Japanese_spaniel/
│       └── ...
└── annotations/
    └── Annotation/
        ├── n02085620-Chihuahua/
        └── ...
```

### Запуск

```bash
python stanford_dogs/lab_regnet_dogs_kaggle.py
```

### Результаты

После выполнения скрипта будут созданы:
- `training_results_kaggle.png` - графики обучения и сравнения моделей
- `pretrained_adam.pth` - веса модели с Adam
- `pretrained_radam.pth` - веса модели с RAdam
- `scratch_adam.pth` - веса модели, обученной с нуля

---


